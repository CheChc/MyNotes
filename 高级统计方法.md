# 高级统计方法

## R语言

```R
lm()  拟合线性回归             多项式回归（ ploy() )函数

coef()  返回系数                   提取向量矩阵

plot()  散点图                    显示树的结构

matrix()  生成矩阵

confint()  得到系数估计值的置信区间

predict()  计算置信区间和预测区间    获得新λ值对应的岭回归系数

residuals()  计算残差

abline()  绘制最小二乘回归直线

par()  显示多张图表

par(mfrow=c(2,2))  把绘图区域划分成2x2的网格面板

cor()  计算所有预测变量两两之间相关系数的矩阵

glm()  拟合逻辑斯蒂回归

lda()  拟合线性判别分析模型

qda()  拟合二次判别分析模型

mean() 预测正确率        求出均值

cv.glm()  用于实现折cv交叉验证逻辑斯蒂回归

is.na()  用于识别有缺失值的观测

regsubset()  实现最优预测变量子集的筛选         method="forward"向前逐步 

														               method="backward"向后逐步

glmnet()  拟合岭回归和lasso模型          参数无α（α=0）为岭回归

                                      参数有α为lasso回归

cv.glmnet()  交叉验证岭回归（默认10折）

pcr()  实现主成分回归

plsr()  拟合片最小二乘回归模型

smooth.splines()  拟合光滑样条

loess()函数  拟合局部回归（光滑样条）

gam()  拟合广义可加模型

text()  显示结点标记

prune.tree()    prune.misclass()  对回归树（分类树）剪枝

svm()  支持向量机分类函数           kernel="linear"拟合径向基核函数

table()  列链表

kmeans()  执行k均值聚类

hclust()  执行系统聚类

scale()  对变量进行标准化处理（用于系统聚类之前）

anova()  进行方差分析

jitter()  增加随机噪音

bs()  默认生成三次样条

ns()  生成自由样条
```

## 第三章：线性回归
### 一、简单线性回归
简单的监督学习方法

#### 单一预测变量

设定X响应y,记为，ßº, ß,,代表斜率和截距，二者为系数或参数。
估计系数：最小二乘估计
残差平方和： RSS

均方误差：mse 

使用最小二乘使RSS最小

#### 假设检验

对零假设：Ho: X和丫没关系
备择假设：H,: X和Y有一定关系
p值：观测值大于等于t的绝对值的概率

### 二、判断拟合质量

#### 一、残差标准误：rse

每个观测都是有误差的，他是响应值会偏离真正回归直线的平均量，RSE越小，拟合的越好

#### 二、R2统计量

R2统计量采取比例的形式，值总在0和1之间，与Y的量级无关。

R2接近1说明可以解释大部分相应变量的变异。

### 三、潜在的问题
#### 非线性、自相关、离群点…
非线性：使用残差图观察。Residual plot,若存在某些规律，则模型可能有问题。加高次项解决
自相关：置信区间估计出现问题，绘制时间函数残差图。
误差项方差非恒定：随响应值增加而增加。

#### 解决方式

离群点：增加了残差标准误，学生化残差大于3为离群点。若为错误，可以删除。
高杠杆点：观测点异常，对回归有很大影响。
在多元回归中，有从整个观测中不寻常。可计算杠杆统计量
共线性：两个变量高度相关。正确检测概率被减小了。可以检测相关系数矩阵，或计算方差膨胀因子

### 四、非线性关系

使用多项式回归

## 第四章：分类
二元情况，线性回归可用（哑变量），但二元以上，则不可用，可能会产生概率小于o大于1的情况
### 逻辑斯蒂回归

混淆现象：观察二者之间的关系以及总体的关系，只用一个预测变量做回归，可能与总体完全不一样。



### 线性判别分析: LDA

形式上与逻辑斯蒂相似，可扩展性更高。
适用于类区分度高时，样本量比较小时
运用贝叶斯定理分类：按概率密度分类：

### 使用LDA的原因

- 类别区分度高的时候，GLM的参数估计不够稳定

- 如果样本量较小，且X近似服从正态分布，那么LDA更稳定

- 响应分类多于两类时，LDA应用更普遍
### 运用贝叶斯定理

  

## 第五章：重抽样方法
交叉验证、自助法
从训练数据中取子集
### 一、验证集的方法
随机分为两部分：训练集、验证集
缺点：估计的波动很大
Sample分成两半，I'm中的subset拟合
### 二、k折交叉验证CV
随机分为k份，选一份作验证集
N折交叉验证：计算大，方差大一使用最小二乘，多项式回归
更好的选择是K=5到10
CV . g l m既建模又交叉验证
5.3自助法bootstrap:估计统计量
使用计算机模拟获得数据集过程
## 第六章：线性模型的选择和正则化

### 一、最优子集选择
对P个预测的所有组合进行最小二乘拟合
效率不高，当P大于四十，不具计算可行性
搜索空间增大，对新数据不具备很好的预测能力
过拟合，系数估计方差高
### 二、向前逐步选择
每次加一个预测变量（从0开始）
效率更高，不保证拿到最优的
最优子集选择方法：
根据过拟合导致的偏差进行调整，间接地估测
Cp，AIC，BIC与调整R2
AIC：赤池信息量准则
BIC：贝叶斯信息准则
### 三、压缩估计
#### 1、岭回归
优势：与最小二乘相比，综合权衡了方差和误差
劣势：最终模型包含全部p个变量，当p个数非常大时，不便于模型解释。
随着入的增加，岭回归的拟合结果的光滑度会降低，方差降低但是偏差增加。
#### 2、lasso

一种相对较新替代岭回归的方法，性质上和岭回归类似，当最小二乘出现较大方差时，lasso可得到更精准的预测结果

lasso采用l1惩罚项而不是l2惩罚项，lasso得到的是稀疏模型，也就是所有变量的一个子集。
lasso可以将系数估计完全压缩至0，但是岭回归不可以。
入为0，lasso等价于最小二乘，入足够大，lasso为零模型。
### 四、降维方法
将预测变量进行转换，然后用转换之后的变量拟合最小二乘模型
#### 1、主成分分析（PCA）
用投影的方法将高维空间压缩到低维
#### 2、主成分回归（PCR）
PCA降维后的特征应用到LR，即将M个主成分作为预测变量，用最小二乘拟合线性回归模型，少数的主成分足以解释大部分的数据波动和数据与相应变量的关系。
#### 3、偏最小二乘（PLS）
将原始变量的线性组合作为新的变量集。
寻找一个可以同时解释相应变量和预测变量的方向。
与最小二乘相比，三种方法都有显著提升，其中主成分分析与岭回归要比lasso更好一些。
## 第七章：非线性模型
### 1､多项式回归：高次幂

### 2、阶梯函数：空间切割成k个不同区域
统称为基函数，原理是对变量X的函数变换
### 3、回归样条
在X的不同区域拟合低阶函数，代替高阶函数

#### 自然样条

附加了边界约束的回归样条

附加的边界条件使自然样条在边界处更稳定

#### 光滑样条

最小化一个带光滑惩罚项的残差平方和的式子

### 4、广义可加模型

每个变量用一个非线性函数替换

##### 选择光滑参数

光滑样条有n个参数，但实际衡量光滑度的标准是df，值越大说明样条越光滑，偏差低，方差高。

## 第八章：基于树的方法
生成树：自上而下、贪婪：top-down greedy
### 1、树的剪枝：
生成很大的树T0，通过剪枝得到子树
最弱联系剪枝

### 2、分类树

也采用递归二叉分裂

确定二叉分裂点的准则：分类错误率

#### 基尼系数

基尼系数越小，代表某个节点的观测值几乎都来自同一类别

#### 互熵

如果第m个节点纯度较高，则互熵越小

### 3、树的优缺点：
1.决策树解释性强
2.更接近人的决策模式
3.可以用图形表示
4.可以直接处理定型的变量而无需使用哑变量
5.树的准确性有可能达不到其他回归的水平
### 4、装袋法：
自助法可以作为代替，从某个单一的训练集中重复取样。对所有预测值取平均。

装袋法对预测准确性的提升是以**牺牲解释性**为代价的。

### 5、随机森林：
对树做去除相关性处理，考虑树上的每一个分裂点，在每个分裂点都进行重新抽样。
提升法：采用类似的方法，但是树均为顺序生成

### 6、提升法

一种舒缓的训练模型的方法，利用现有模型的残差生成决策树，再把新生成的树加到相应的函数。

### 总结：
原理均为在训练数据上生成大量的树，然后将生成树进行组合，从而得出预测结果。
后两种方法都是先进的监督学习方法，但是结果可能很难解释。
## 第九章：支持向量机
### 1、最大间隔分类器
#### 约束优化问题

所有的分离超平面中，找出使两类之间的间隙或间隔最大的超平面。

### 2、支持向量分类器

也称为软间隔分类器

最大化了软间隔-调节参数C，能容忍的穿过间隔，C通过交叉验证来选择

只有落在*间隔和穿过间隔*的观测会影响平面，落在正确间隔之外的观测没有任何影响

刚好落在**间隔和落在间隔错误一侧**的观测叫支持向量

### 3、核函数（kernel）和支持向量机（SVM）

支持向量分类器的一个拓展，使用一种特殊的方式：核函数

为了适应类别之间的非线性边界而扩大特征空间

## 第十章：无指导学习
### 1、主成分分析

计算主成分并使用主成分理解数据的一种方法

是一种无指导学习方法

### 2、聚类分析方法

在一个数据集中寻找子群或类的技术

#### 两种聚类算法：

##### k均值聚类：

把观测分割到k个类中，使得k个类总的类内差异尽可能小。
##### 系统聚类法：

从谱系图的底部开始，n个观测各自都看成一类

把两个最为相似的类汇合到一起，如此进行下去，知道所有观测都属于某一个类时停止。

可以输出一个迷你的有关各个观测的树形表示，称为**谱系图**。

### 3、四种常用的距离形式：

1.最长距离法：最大类间相异度
2.最短距离法：最小相异度
3.类平均法：平均相异度
4.重心法：会导致不良的倒置现象发生。